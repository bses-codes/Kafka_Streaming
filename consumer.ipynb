{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Author: Bishesh Kafle\n",
    "##### Date : 2024-07-22"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:03:51.669252Z",
     "start_time": "2024-07-22T11:03:51.661398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:03:54.068897Z",
     "start_time": "2024-07-22T11:03:52.910820Z"
    }
   },
   "source": [
    "# Define Kafka topic name and bootstrap server address\n",
    "kafka_topic_name = 'Topic1'\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "# Initialize a Spark session with Kafka support\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Structured Streaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set logging level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic_name) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Print the schema of the incoming Kafka messages\n",
    "df.printSchema()\n",
    "\n",
    "# Select the 'value' and 'timestamp' fields from the Kafka message and cast 'value' to string\n",
    "df1 = df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")\n",
    "\n",
    "# Define the schema of the incoming CSV data\n",
    "df_schema_string = \"order_id INT, account_number STRING, branch STRING, transaction_code STRING\"\n",
    "\n",
    "# Parse the 'value' field from CSV format into individual columns based on the schema\n",
    "df2 = df1 \\\n",
    "    .select(from_csv(col(\"value\"), df_schema_string) \\\n",
    "    .alias(\"data\"), \"timestamp\")\n",
    "\n",
    "# Flatten the 'data' structure to select individual fields along with the timestamp\n",
    "df3 = df2.select(\"data.*\", \"timestamp\")\n",
    "\n",
    "# Create a temporary view to allow for SQL queries on the processed data\n",
    "df3.createOrReplaceTempView(\"proc_rw_transaction_data\")\n",
    "\n",
    "# Execute an SQL query to select all data from the temporary view\n",
    "data = spark.sql(\"SELECT * FROM proc_rw_transaction_data\")\n",
    "\n",
    "# Write the streaming query results to an in-memory table for further processing or visualization\n",
    "data_agg_write_stream = data \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"temp_stream_data\") \\\n",
    "    .start()\n",
    "\n",
    "# Await termination for a short period to allow the stream to start processing\n",
    "data_agg_write_stream.awaitTermination(1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:03:57.827985Z",
     "start_time": "2024-07-22T11:03:57.780927Z"
    }
   },
   "source": [
    "# Write the results of query to dataframe\n",
    "df = spark.sql(\"SELECT * FROM temp_stream_data\")\n",
    "df.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------+----------------+--------------------+\n",
      "|order_id|     account_number|branch|transaction_code|           timestamp|\n",
      "+--------+-------------------+------+----------------+--------------------+\n",
      "|       0|02XYZXYZ10017529992|    15|              CI|2024-07-22 13:04:...|\n",
      "|       1|02XYZXYZ10017529992|    15|              CI|2024-07-22 13:04:...|\n",
      "|       2|02XYZXYZ10017517823|    15|              CI|2024-07-22 13:04:...|\n",
      "|       3|02XYZXYZ10017517823|    15|              CI|2024-07-22 13:04:...|\n",
      "|       4|02XYZXYZ10017517823|    15|              CI|2024-07-22 13:04:...|\n",
      "|       5|02XYZXYZ10017519116|    15|              CI|2024-07-22 13:04:...|\n",
      "|       6|02XYZXYZ10017519121|    15|              CI|2024-07-22 13:04:...|\n",
      "|       7|02XYZXYZ10017520752|    15|              CI|2024-07-22 13:04:...|\n",
      "|       8|02XYZXYZ10017520752|    15|              CI|2024-07-22 13:04:...|\n",
      "|       9|02XYZXYZ10017520752|    15|              CI|2024-07-22 13:04:...|\n",
      "|      10|02XYZXYZ10017555515|    15|              CI|2024-07-22 13:04:...|\n",
      "|      11|02XYZXYZ10017555612|    15|              CI|2024-07-22 13:04:...|\n",
      "|      12|02XYZXYZ10017557070|    15|               O|2024-07-22 13:04:...|\n",
      "|      13|02XYZXYZ10017557090|    15|              CI|2024-07-22 13:04:...|\n",
      "|      14|02XYZXYZ10017557096|    15|              CI|2024-07-22 13:04:...|\n",
      "|      15|02XYZXYZ10017503121|    15|              CI|2024-07-22 13:04:...|\n",
      "|      16|02XYZXYZ10017557935|    15|              CI|2024-07-22 13:04:...|\n",
      "|      17|02XYZXYZ10017557935|    15|              CI|2024-07-22 13:04:...|\n",
      "|      18|02XYZXYZ10017557936|    15|              CI|2024-07-22 13:04:...|\n",
      "|      19|02XYZXYZ10017557936|    15|              CI|2024-07-22 13:04:...|\n",
      "+--------+-------------------+------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:04:00.973115Z",
     "start_time": "2024-07-22T11:04:00.894829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the values from dataframe df to stream_data table in kafka_con database\n",
    "from mysql_connection import *\n",
    "df_table(df.toPandas(),'kafka_con','stream_data')"
   ],
   "outputs": [],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
